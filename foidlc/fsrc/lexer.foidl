;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
; lexer
; Compiler lexical scanner
;
; Copyright (c) Frank V. Castellucci
; All Rights Reserved
;
; Licensed under the Apache License, Version 2.0 (the "License");
; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     http://www.apache.org/licenses/LICENSE-2.0
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS,
; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
; See the License for the specific language governing permissions and
; limitations under the License.
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

module lexer

include langcorem

; Using RTL regex_match to construct the foidlc lexer

var patterns [
    {
        :type   :module
        :regex  regex: "module"
    }
    {
        :type   :include
        :regex  regex: "include$"
    }
    {
        :type   :variable
        :regex  regex: "var$"
    }
    {
        :type   :function
        :regex  regex: "func$"
    }
    {
        :type   :private
        :regex  regex: ":private"
    }
    {
        :type   :let
        :regex  regex:"let"
    }
    {
        :type   :match
        :regex  regex: "match"
    }
    {
        :type   :match_guard
        :regex  regex: "\|"
    }
    {
        :type   :match_exprref
        :regex  regex: "%0"
    }
    {
        :type   :default
        :regex  regex: ":default"
    }
    {
        :type   :group
        :regex  regex: "@\("
    }
    {
        :type   :lambda
        :regex  regex: "\^"
    }
    {
        :type   :lparen
        :regex  regex: "\("     ;   Used for partials
    }
    {
        :type   :rparen
        :regex  regex: "\)"
    }
    {
        :type   :math_call
        :regex  regex: "(\+|-|/|\*):{1,1}?"
    }
    {
        :type   :boolpred_call
        :regex  regex: "(=|>=|<=|>|<|not=):"
    }
    {
        :type   :lbracket
        :regex  regex: "\["     ;   Used for list
    }
    {
        :type   :rbracket
        :regex  regex: "\]"
    }
    {
        :type   :langle
        :regex  regex: "<"      ;   Used for vector
    }
    {
        :type   :rangle
        :regex  regex: ">"
    }
    {
        :type   :lbrace         ;   Used for maps
        :regex  regex: "\{"
    }
    {
        :type   :rbrace
        :regex  regex: "\}"
    }
    {
        :type   :lset
        :regex  regex: "#\{"    ;   Used for sets
    }
    {
        :type   :funccall
        :regex  regex: "[a-zA-Z]([a-zA-Z0-9_]*)?:"
    }
    {
        :type   :funccall_pred
        :regex  regex: "[a-zA-Z]([a-zA-Z0-9_]*)?\?:"
    }
    {
        :type   :funccall_bang
        :regex  regex: "[a-zA-Z]([a-zA-Z0-9_]*)?!:"
    }
    {
        :type   :keyword
        :regex  regex: ":[a-zA-Z]([a-zA-Z0-9_]*)?"
    }
    {
        :type   :real_number
        :regex  regex: "-?(?:0|[1-9]\d*)\.\d*(?:[eE][+\-]?\d+)?"
    }
    {
        :type   :real_number
        :regex  regex: "-?[0-9]+?\.[0-9]+?"
    }
    {
        :type   :bit
        :regex  regex: "0[bB][0-1]+"

    }
    {
        :type   :hex
        :regex  regex: "0[xX][0-9a-fA-F]+"
    }
    {
        :type   :integer
        :regex  regex: "-?[0-9]+"
    }
    {
        :type   :math_ref
        :regex  regex: "(\+|-|/|\*)"
    }
    {
        :type   :char
        :regex  regex:  "\'[0-9a-zA-Z]*?\'"
    }
    {
        :type   :symbol_pred
        :regex  regex: "[a-zA-Z]([a-zA-Z0-9_]*)?\?"
    }
    {
        :type   :symbol_bang
        :regex  regex: "[a-zA-Z]([a-zA-Z0-9_]*)?!"
    }
    {
        :type   :symbol
        :regex  regex: "[a-zA-Z]([a-zA-Z0-9_]*)?"
    }
    {
        :type   :if_expr
        :regex  regex: "\?:"
    }
    {
        :type   :comment
        :regex  regex: ";.*"
    }
]

;   List  of ignores
var ignores [
    {
        :type   :ignore
        :regex  regex: "[,\t ]+"
    }
]

; Function: clean_tokens
; Description: Eliminates token types matching in dropset
;   toks - collection of tokens
;   dropset - a set of one or more token_type keywords to drop from results
func :private clean_tokens [toks dropset]
    fold:
        ^[acc el]
            ?: get: dropset get: el :token_type
                acc
                list_extend!: acc el
        [] toks

; Function: process_tokens
; Description: Takes results of tokenize and prepare result
; Syntax: process_tokens: toks dropset
;   toks - collection of tokens
;   dropset - a set of one or more token_type keywords to drop from results
;   def_fn - function to process succesful scan
;   keep_empty - Prevents fail on empty token file

func :private process_tokens [source_file toks dropset def_fn keep_empty]
    match count: toks
    | 0     @(
                print!: "Warning: file: "
                print!: source_file
                printnl!: " is empty. "
                ?: keep_empty
                    toks
                    fail: )
    | 1     @(
                print!: "Error: token scan halted: "
                printnl!: get: toks zero
                fail: )
    | :default (def_fn toks dropset)

; Function: lexscan
; Description: Tokenizes source content and prepare results by
; calling provided function reference
; Syntax: lexscan: source_file dropset def_fn
;   source_file - fully qualified source file name
;   dropset - a set of one or more token_types to drop from results
;   def_fn - Function reference taking token list and dropset arguments
;   keep_empty - Prevents fail on empty token file

func :private lexscan [source_file dropset def_fn keep_empty]
    let [src_string quaf!: source_file
         toks tokenize: src_string patterns ignores]
         process_tokens: source_file toks dropset def_fn keep_empty

; Function: lex
; Description: Perform a lexical scan of source and
; returns a top down ordered list of tokens.
; Checks tokens for error and, if found, prints error and exits
; Syntax: lex: source_file dropset
;   source_file - valid foidl source or header file
;   dropset - a set of one or more token_type keywords to drop from results
;   keep_empty - Prevents fail on empty token file

func lex [source_file dropset keep_empty]
    lexscan: source_file dropset clean_tokens keep_empty

